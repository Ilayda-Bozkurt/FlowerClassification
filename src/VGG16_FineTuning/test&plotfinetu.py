# -*- coding: utf-8 -*-
"""test&plotFineTu.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cvcd35AtdqqeJQbEFzJQKJIZqYgO2S7g
"""

# Function to evaluate the model
def eval(model, X, Y):
    model.evaluate(X, Y)

# Function for making predictions
def prediction(model, X, Y):
    Y_predict = model.predict(X)
    Y_pred = [np.argmax(i) for i in Y_predict]
    print('Predicted the first five labels:', Y_pred[:5])
    print('True labels of the first five elements:', Y[:5])
    return Y_pred

# Function to display classification report and confusion matrix
def report(truth, predictions):
    print(classification_report(truth, predictions))
    cm = tf.math.confusion_matrix(labels=truth, predictions=predictions)
    plt.figure(figsize=(10, 7))
    sn.heatmap(cm, annot=True, fmt='d')
    plt.xlabel('Predicted')
    plt.ylabel('Truth')

# Function to plot training and validation loss
def plot_loss(history):
    plt.figure(figsize=(10, 6))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid()
    plt.show()

# Function to plot training and validation accuracy
def plot_accuracy(history):
    plt.figure(figsize=(10, 6))
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid()
    plt.show()

# Visualization of the Educational Process
plot_loss(history_finetuned)
plot_accuracy(history_finetuned)

# Evaluation on Test Set
eval(model_finetuned, X_test, Y_test)

# Predictions and Classification Report
preds = prediction(model_finetuned, X_test, Y_test)
report(Y_test, preds)

from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

#This function takes predictions from test data and returns them with the actual labels.
def get_all_preds(model, loader):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for images, labels in loader:
            images = images.to(device)
            labels = labels.to(device)
            outputs = model(images)
            _, preds = torch.max(outputs, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    return np.array(all_preds), np.array(all_labels)

#Get predictions
preds, labels = get_all_preds(flower_cnn, test_dl)

#Performance report
print("Classification Report:\n")
print(classification_report(labels, preds, target_names=original_dataset.classes))

import seaborn as sns
import matplotlib.pyplot as plt

#confusion matrix visualization
cm = confusion_matrix(labels, preds)
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', xticklabels=original_dataset.classes, yticklabels=original_dataset.classes, cmap="Blues") # Use original_dataset.classes
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()