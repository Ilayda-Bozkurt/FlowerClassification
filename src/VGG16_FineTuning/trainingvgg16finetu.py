# -*- coding: utf-8 -*-
"""trainingVGG16FineTu.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/128W1Lxw3bpz4zQwhv-1YffoZne1-Q4MS
"""

# Creating the Model
model_finetuned = my_model_finetuned()

# Defining TensorBoard Callback
tb_callback = tf.keras.callbacks.TensorBoard(log_dir='/content/drive/logs_finetuned', histogram_freq=1)

# Training the Model
history_finetuned = model_finetuned.fit(
    X_train, Y_train,
    epochs=20,
    validation_split=0.2,
    callbacks=[tb_callback]
)

#This function is used to get the learning rate of an optimizer in PyTorch.
def get_lr(optimizer):
    for param_group in optimizer.param_groups:
        return param_group['lr']

#This function trains a PyTorch model using a 1-cycle learning rate policy along with a training and validation process.

def fit_one_cycle(epochs:int,
                  max_lr:float,
                  model:object,
                  train_loader:DataLoader,
                  val_loader:DataLoader,
                  weight_decay:float=0,
                  grad_clip:bool=None,
                  opt_func=torch.optim.SGD) -> list[dict]:
    #Clears GPU memory.istory list to store results from each epoch.
    torch.cuda.empty_cache()
    history = []

    # Set up custom optimizer with weight decay
    optimizer = opt_func(model.parameters(),
                         max_lr,
                         weight_decay=weight_decay)

    # Set up one-cycle learning rate scheduler
    lmbda = lambda epoch: 0.65 ** epoch
    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs,
                                                steps_per_epoch=len(train_loader))


    #The model is trained for a specified number of epochs.
    for epoch in range(epochs):
        print(f"Epoch {epoch} started")

        #Puts the model into training mode.
        model.train()
        train_losses = []
        correct = 0
        total = 0
        lrs = []

        for batch in train_loader:
            loss = model.training_step(batch)
            train_losses.append(loss)
            loss.backward()

            #Gradients are limited so that they do not exceed a certain value.
            if grad_clip:
                nn.utils.clip_grad_value_(model.parameters(), grad_clip)

            #Model parameters are updated.
            optimizer.step()
            optimizer.zero_grad()
            lrs.append(get_lr(optimizer))
            sched.step()

            #Calculate the train accuracy
            images,labels = batch
            outputs = model(images)
            _, preds = torch.max(outputs,dim = 1)
            correct += torch.sum(preds == labels).item()
            total += len(labels)
        train_acc = correct/total

        #Validation loss and accuracy are calculated by calling the evaluate() function.
        result = evaluate(model, val_loader)
        result['train_acc'] = train_acc
        result['train_loss'] = torch.stack(train_losses).mean().item()
        result['lrs'] = lrs
        model.epoch_end(epoch, result)
        history.append(result)
    return history

#We calculate the number of classes using the list named classes.
num_classes = len(classes) # 5
#The flower_cnn model is ported to run on the specified device (e.g. GPU or CPU)
flower_cnn = to_device(FlowersCNNModel(), device=device)

#The result is added to the list named history.
history = [evaluate(flower_cnn, val_dl)]
history

#Using SGD optimization, it adjusts some of the parameters necessary to start training the model.
flower_cnn = to_device(FlowersCNNModel(), device=device)
epochs = 25
max_lr = 0.01
grad_clip = 0.1
weight_decay = 1e-4

#As training is completed, the model's loss and accuracy at each epoch are collected in the history list.
history += fit_one_cycle(epochs, max_lr, flower_cnn, train_dl, val_dl,
                             grad_clip=grad_clip,
                             weight_decay=weight_decay)