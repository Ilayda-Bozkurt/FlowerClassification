# -*- coding: utf-8 -*-
"""dataloaderCNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HQXgZ5orQT4CSwQSGKKQDRukkXzTjo4T
"""

# Imports
import os
import torch
import torchvision
import tarfile
import torch.nn as nn
import numpy as np
import torch.nn.functional as F
import opendatasets as od
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader, ConcatDataset
import torchvision.transforms as tt
from torch.utils.data import random_split
from torchvision.utils import make_grid
import matplotlib
import matplotlib.pyplot as plt
import jovian
import matplotlib.image as mpimg
from sklearn.model_selection import train_test_split

matplotlib.rcParams['figure.facecolor'] = '#ffffff'

#Define the URL of the dataset
dataset_url = "https://www.kaggle.com/datasets/alxmamaev/flowers-recognition?datasetId=8782&sortBy=voteCount"

# Determining data directory path
data_dir = './flowers-recognition/flowers'  #It is determined where the data set will be located in the working environment.

#If folder does not exist download dataset
if not os.path.isdir(data_dir):
    od.download(dataset_url)

#List subfolders within folder
classes = os.listdir(data_dir)
print(classes)

#Data transforms (normalization & data augmentation)

#Images will be resized to 150x150 pixels
IMAGE_RESIZE = (150, 150)

#This function works on all images in the given dataset_path and calculates mean and std.
def calculate_mean_std(dataset_path):

#0 = Num of examples in batch,  2 = height,  3 = wight
#The average is taken in these dimensions.
    dim = [0, 2, 3]

    #Using tt, each image is resized to 150x150 and converted to tensor(data transform)
    x_form = tt.Compose([tt.Resize(IMAGE_RESIZE),
                        tt.ToTensor()
                        ])

    ##Defining the dataset and data loader
    dataset = ImageFolder(data_dir, transform=x_form)
    dataset_loader = DataLoader(dataset=dataset, batch_size=512, shuffle=True, num_workers=8)

    ##Initialize average and squared average sums to collect average values ​​from channels in each batch
    channels_sum, channels_squared_sum, num_batches = 0, 0, 0

    #Calculate mean and squared mean for each batch
    for data, _ in dataset_loader:
        channels_sum += torch.mean(data, dim=dim)
        channels_squared_sum += torch.mean(data**2, dim=dim)
        num_batches += 1

    #Calculate total mean and standard deviation
    mean = channels_sum/num_batches
    std = np.sqrt(channels_squared_sum/num_batches - np.square(mean))

    return mean, std

#Converting tensors to Python tuples
mean_std = calculate_mean_std(data_dir)
mean = tuple(scaler.item() for scaler in mean_std[0])
std = tuple(scaler.item() for scaler in mean_std[1])
stats = (mean, std)
stats

#Data transforms (normalization & data augmentation)

#normalized original data
transformer = {
    'original' : tt.Compose([
        tt.Resize(IMAGE_RESIZE),
        tt.ToTensor(),
        tt.Normalize(*stats,inplace=True)
    ]),

    #light rotation, panning and mirroring
    'dataset1' : tt.Compose([
        tt.Resize(IMAGE_RESIZE),
        tt.RandomAffine(degrees=11, translate=(0.1,0.1), scale=(0.8,0.8)),
        tt.RandomHorizontalFlip(),
        tt.ToTensor(),
        tt.Normalize(*stats,inplace=True)
    ]),

    #rotation, 20% chance of reflection
    'dataset2' : tt.Compose([
        tt.Resize(IMAGE_RESIZE),
        tt.RandomRotation(5),
        tt.RandomHorizontalFlip(p=0.2),
        tt.ToTensor(),
        tt.Normalize(*stats,inplace=True)
    ]),

    #color change, rotation
    'dataset3' : tt.Compose([
        tt.Resize(IMAGE_RESIZE),
        tt.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
        tt.RandomRotation(20),
        tt.RandomHorizontalFlip(p=0.5),
        tt.ToTensor(),
        tt.Normalize(*stats,inplace=True)
    ]),
}

#splits the image dataset into training, validation, and testing sets
original_dataset = ImageFolder(data_dir, transform=transformer['original'])
train_val_dataset, test_ds = train_test_split(original_dataset, test_size=0.2, shuffle=True, random_state=42)
train_dataset, val_ds = train_test_split(train_val_dataset, test_size=0.1, shuffle=True, random_state=42)

#create train and val dataset from remaining and  augmented dataset
#This process ensures that the model is robust to different variations by using different versions of the images.
train_ds = ConcatDataset([train_dataset,
                        ImageFolder(data_dir, transform=transformer['dataset1']),
                        ImageFolder(data_dir, transform=transformer['dataset2']),
                        ImageFolder(data_dir, transform=transformer['dataset3']),
                        ])

#determines how many images the model will process at each training step
BATCH_SIZE=64

#dataLoader allows us to feed data in mini-batch during model training.
#Training data loader
train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)

#validation data loader
val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE*2, num_workers=2, pin_memory=True)

#This function visualizes normalized training dataset.
def show_batch(dl):
    for images, labels in dl:
        fig, ax = plt.subplots(figsize=(12, 6))
        ax.set_xticks([]); ax.set_yticks([])
        ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))
        break

#Its purpose is to automatically move the training data and model to the CPU or GPU and speed up the training process.
#If GPU is available it will be used, otherwise CPU will be used
def get_default_device():
    if torch.cuda.is_available():
        return torch.device('cuda')
    else:
        return torch.device('cpu')

def to_device(data, device):
    if isinstance(data, (list,tuple)):
        return [to_device(x, device) for x in data]
    return data.to(device, non_blocking=True)

class DeviceDataLoader():
    def __init__(self, dl, device):
        self.dl = dl
        self.device = device

    def __iter__(self):
        for b in self.dl:
            yield to_device(b, self.device)

    def __len__(self):
        return len(self.dl)

#device is an object used in PyTorch to specify which hardware tensors and models will run on.
device = get_default_device()
device

#It wraps data loaders with a special class, DeviceDataLoader, that will automatically move the data to the GPU or CPU.
train_dl = DeviceDataLoader(train_dl, device)
val_dl = DeviceDataLoader(val_dl, device)